{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMRZaTbWpnBn"
      },
      "source": [
        "# Import the package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hDOtD43ECOG",
        "outputId": "35d0a34e-7120-416c-dda6-98c30bd2323b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install demoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO1yMPAOmjdS",
        "outputId": "15ca334e-ad1d-4d45-8b04-26859a3f9c7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.4.0\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4.0) (4.1.1)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVMTz-gOoGF_",
        "outputId": "c3151905-04bf-4fda-9bca-4463711ca349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "import csv\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('all')\n",
        "import demoji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycqWYfXQFzjN"
      },
      "source": [
        "# Connect to the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPueq-Mp0cJZ",
        "outputId": "d1c6cd7c-db11-4918-df2a-0b319929c122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/google_drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/google_drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_BnOfuv0iKx",
        "outputId": "ae024bf1-693c-4fc6-97e0-3db26f57f074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/google_drive\n",
            "\u001b[0m\u001b[01;34mMyDrive\u001b[0m/\n",
            "/content/google_drive/MyDrive\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/                            output.gsheet\n",
            " covid-19-vaccines-sentiment-analysis.ipynb   output.txt\n",
            " download_tweets.py                           \u001b[01;34mSummary_Details_files\u001b[0m/\n",
            " final_ids.txt                                test.csv\n",
            " \u001b[01;34mhydrated\u001b[0m/                                    train.csv\n",
            " MEST-CoV-test.csv                            \u001b[01;34mTweet_IDs\u001b[0m/\n",
            " METS-CoV-dev.csv                             \u001b[01;34mTweet_Summary\u001b[0m/\n",
            " METS-CoV-train.csv                           vaccination_all_tweets.csv\n",
            " output.csv                                   vaccination_all_tweets.gsheet\n",
            "/content/google_drive/MyDrive/Colab Notebooks\n",
            " aaai_covid_fake_BERT_pytorch.ipynb      en_core_web_sm-3.0.0.tar.gz.2\n",
            " aaai_CT_BERT.ipynb                      en_core_web_sm-3.0.0.tar.gz.3\n",
            " aaai_ensembling.ipynb                   en_core_web_sm-3.0.0.tar.gz.4\n",
            " Abbreviations.txt                       en_core_web_sm-3.0.0.tar.gz.5\n",
            " archive.zip                             en_core_web_sm-3.0.0.tar.gz.6\n",
            " BERT.ipynb                              en.csv\n",
            " bertmodel.pth                           glove.840B.300d.txt\n",
            " covid-19-tweets.ipynb                   LSTM.ipynb\n",
            " download_tweets.py                      “LSTM.ipynb”的副本\n",
            " EDA_finalversion.ipynb                  Preprocess.ipynb\n",
            " EDA.ipynb                               preprocess.py\n",
            "'EDA + Preprocessing + Modeling.ipynb'   test.csv\n",
            " en_core_web_sm-3.0.0.tar.gz             train.csv\n",
            " en_core_web_sm-3.0.0.tar.gz.1           wordReplace.py\n"
          ]
        }
      ],
      "source": [
        "%cd google_drive/\n",
        "\n",
        "%ls\n",
        "%cd MyDrive/\n",
        "%ls\n",
        "%cd 'Colab Notebooks'/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsYkDld0ayGW",
        "outputId": "f25d79c8-fd3f-4fa3-c1c3-c571296a580f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whole number of dataset = 10000\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "df = pd.read_csv('/content/google_drive/MyDrive/Colab Notebooks/en.csv')\n",
        "print(\"whole number of dataset = {}\".format(len(df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbpibQyBkfHZ"
      },
      "source": [
        "# build model for train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNaC0P7wC5Ak",
        "outputId": "225eac13-324f-415e-d097-597bb2f01f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'google_drive/'\n",
            "/content/google_drive/MyDrive/Colab Notebooks\n",
            " aaai_covid_fake_BERT_pytorch.ipynb      en_core_web_sm-3.0.0.tar.gz.2\n",
            " aaai_CT_BERT.ipynb                      en_core_web_sm-3.0.0.tar.gz.3\n",
            " aaai_ensembling.ipynb                   en_core_web_sm-3.0.0.tar.gz.4\n",
            " Abbreviations.txt                       en_core_web_sm-3.0.0.tar.gz.5\n",
            " archive.zip                             en_core_web_sm-3.0.0.tar.gz.6\n",
            " BERT.ipynb                              en.csv\n",
            " bertmodel.pth                           glove.840B.300d.txt\n",
            " covid-19-tweets.ipynb                   LSTM.ipynb\n",
            " download_tweets.py                      “LSTM.ipynb”的副本\n",
            " EDA_finalversion.ipynb                  Preprocess.ipynb\n",
            " EDA.ipynb                               preprocess.py\n",
            "'EDA + Preprocessing + Modeling.ipynb'   test.csv\n",
            " en_core_web_sm-3.0.0.tar.gz             train.csv\n",
            " en_core_web_sm-3.0.0.tar.gz.1           wordReplace.py\n",
            "[Errno 2] No such file or directory: 'MyDrive/'\n",
            "/content/google_drive/MyDrive/Colab Notebooks\n",
            " aaai_covid_fake_BERT_pytorch.ipynb      en_core_web_sm-3.0.0.tar.gz.2\n",
            " aaai_CT_BERT.ipynb                      en_core_web_sm-3.0.0.tar.gz.3\n",
            " aaai_ensembling.ipynb                   en_core_web_sm-3.0.0.tar.gz.4\n",
            " Abbreviations.txt                       en_core_web_sm-3.0.0.tar.gz.5\n",
            " archive.zip                             en_core_web_sm-3.0.0.tar.gz.6\n",
            " BERT.ipynb                              en.csv\n",
            " bertmodel.pth                           glove.840B.300d.txt\n",
            " covid-19-tweets.ipynb                   LSTM.ipynb\n",
            " download_tweets.py                      “LSTM.ipynb”的副本\n",
            " EDA_finalversion.ipynb                  Preprocess.ipynb\n",
            " EDA.ipynb                               preprocess.py\n",
            "'EDA + Preprocessing + Modeling.ipynb'   test.csv\n",
            " en_core_web_sm-3.0.0.tar.gz             train.csv\n",
            " en_core_web_sm-3.0.0.tar.gz.1           wordReplace.py\n",
            "[Errno 2] No such file or directory: 'Colab Notebooks/'\n",
            "/content/google_drive/MyDrive/Colab Notebooks\n",
            " aaai_covid_fake_BERT_pytorch.ipynb      en_core_web_sm-3.0.0.tar.gz.2\n",
            " aaai_CT_BERT.ipynb                      en_core_web_sm-3.0.0.tar.gz.3\n",
            " aaai_ensembling.ipynb                   en_core_web_sm-3.0.0.tar.gz.4\n",
            " Abbreviations.txt                       en_core_web_sm-3.0.0.tar.gz.5\n",
            " archive.zip                             en_core_web_sm-3.0.0.tar.gz.6\n",
            " BERT.ipynb                              en.csv\n",
            " bertmodel.pth                           glove.840B.300d.txt\n",
            " covid-19-tweets.ipynb                   LSTM.ipynb\n",
            " download_tweets.py                      “LSTM.ipynb”的副本\n",
            " EDA_finalversion.ipynb                  Preprocess.ipynb\n",
            " EDA.ipynb                               preprocess.py\n",
            "'EDA + Preprocessing + Modeling.ipynb'   test.csv\n",
            " en_core_web_sm-3.0.0.tar.gz             train.csv\n",
            " en_core_web_sm-3.0.0.tar.gz.1           wordReplace.py\n"
          ]
        }
      ],
      "source": [
        "%cd google_drive/\n",
        "\n",
        "%ls\n",
        "%cd MyDrive/\n",
        "%ls\n",
        "%cd 'Colab Notebooks'/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9QRSYHpGP6g",
        "outputId": "964bf974-a4a9-4097-dc0e-564669d6edf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 97.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 75.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpUkd5Y13uWe"
      },
      "outputs": [],
      "source": [
        "#!pip install torchtext==0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ayhi5jBGMNs"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "SEED = 1024\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import operator\n",
        "\n",
        "from sklearn.metrics import hamming_loss, jaccard_score, label_ranking_average_precision_score, f1_score\n",
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBOiPmpokh0t"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(df, train_size = 0.8, random_state = 1024)\n",
        "\n",
        "df_train.to_csv(\"train.csv\", index = False)\n",
        "df_test.to_csv(\"test.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6o32On8mqpp"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/google_drive/MyDrive/Colab Notebooks/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO0H_qPHmnlt",
        "outputId": "bb6d80cf-5351-40c3-e5e4-c42318c8541c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 8000 entries, 653 to 6075\n",
            "Data columns (total 13 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   ID               8000 non-null   float64\n",
            " 1   Tweet            8000 non-null   object \n",
            " 2   Optimistic       8000 non-null   int64  \n",
            " 3   Thankful         8000 non-null   int64  \n",
            " 4   Empathetic       8000 non-null   int64  \n",
            " 5   Pessimistic      8000 non-null   int64  \n",
            " 6   Anxious          8000 non-null   int64  \n",
            " 7   Sad              8000 non-null   int64  \n",
            " 8   Annoyed          8000 non-null   int64  \n",
            " 9   Denial           8000 non-null   int64  \n",
            " 10  Official report  8000 non-null   int64  \n",
            " 11  Surprise         8000 non-null   int64  \n",
            " 12  Joking           8000 non-null   int64  \n",
            "dtypes: float64(1), int64(11), object(1)\n",
            "memory usage: 875.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LsYXfcanuH5",
        "outputId": "a4a181b1-dbe7-47d3-c378-d87ee832b93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-18 20:07:55--  https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/ad801100-63d4-11eb-8f94-f2d2e139545f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221018%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221018T200756Z&X-Amz-Expires=300&X-Amz-Signature=0a71cab26efccfc576804473eab83207d46dd47c66f0ccd7b6904b806e8d3ce5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.0.0.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-10-18 20:07:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/84940268/ad801100-63d4-11eb-8f94-f2d2e139545f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221018%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221018T200756Z&X-Amz-Expires=300&X-Amz-Signature=0a71cab26efccfc576804473eab83207d46dd47c66f0ccd7b6904b806e8d3ce5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=84940268&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.0.0.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13701062 (13M) [application/octet-stream]\n",
            "Saving to: ‘en_core_web_sm-3.0.0.tar.gz.7’\n",
            "\n",
            "en_core_web_sm-3.0. 100%[===================>]  13.07M  6.19MB/s    in 2.1s    \n",
            "\n",
            "2022-10-18 20:07:59 (6.19 MB/s) - ‘en_core_web_sm-3.0.0.tar.gz.7’ saved [13701062/13701062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D06lO3dQoEBN",
        "outputId": "ab0d5335-9b64-49b6-8417-2e699dc2c9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./en_core_web_sm-3.0.0.tar.gz\n",
            "Collecting spacy<3.1.0,>=3.0.0\n",
            "  Downloading spacy-3.0.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 19.5 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.64.1)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "  Downloading thinc-8.0.17-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 81.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.21.6)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.0.0-py3-none-any.whl size=13704318 sha256=5dd683586cdc03630a26764312dab12094453b11199a2134e9f7c4a5f19b004c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/ce/00/bfa55105a9b450502fca6e1b7cb0c8ada9b344a2f637ab2d9f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: typing-extensions, typer, pydantic, thinc, spacy, en-core-web-sm\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.4.2\n",
            "    Uninstalling typer-0.4.2:\n",
            "      Successfully uninstalled typer-0.4.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.9.2\n",
            "    Uninstalling pydantic-1.9.2:\n",
            "      Successfully uninstalled pydantic-1.9.2\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.4\n",
            "    Uninstalling thinc-8.1.4:\n",
            "      Successfully uninstalled thinc-8.1.4\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.4.0\n",
            "    Uninstalling en-core-web-sm-3.4.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.4.0\n",
            "Successfully installed en-core-web-sm-3.0.0 pydantic-1.8.2 spacy-3.0.8 thinc-8.0.17 typer-0.3.2 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install en_core_web_sm-3.0.0.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8MM_reKoN6Q",
        "outputId": "3c4143be-3175-42ef-9fd7-dc7df7613aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 33.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.17)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.64.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTLrkD7GoXCb",
        "outputId": "63638b69-7f5e-4f72-e39a-03e5578e3557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==3.0.0\n",
            "  Downloading spacy-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 20.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (8.0.17)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (3.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (0.7.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (21.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (4.13.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (0.6.2)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.0) (2.4.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy==3.0.0) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy==3.0.0) (5.2.1)\n",
            "Installing collected packages: pydantic, spacy\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.8.2\n",
            "    Uninstalling pydantic-1.8.2:\n",
            "      Successfully uninstalled pydantic-1.8.2\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.0.8\n",
            "    Uninstalling spacy-3.0.8:\n",
            "      Successfully uninstalled spacy-3.0.8\n",
            "Successfully installed pydantic-1.7.4 spacy-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy==3.0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-mhzCbsQoe9t",
        "outputId": "b216bbc8-e091-499b-d55a-0f206866ec55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.0.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import spacy\n",
        "spacy.__version__\n",
        "#输出\n",
        "'3.0.0'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sms5ECSx6q9t",
        "outputId": "c8fd1d12-aa7d-462d-e6d7-49f7cc913898"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.4.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import torchtext\n",
        "torchtext.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLRlcNmQm9Ux"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "#from torchtext.legacy.data import Field\n",
        "import spacy\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenizer(tweet):\n",
        "    tweet = re.sub(r'[\\n]', ' ', tweet)\n",
        "    return [tok.text for tok in spacy_en.tokenizer(tweet)]\n",
        "\n",
        "TWEET = torchtext.data.Field(sequential = True, lower = True, tokenize = tokenizer)\n",
        "LABEL = torchtext.data.Field(sequential = False, use_vocab = False)\n",
        "\n",
        "dataFields = [(\"ID\", None), (\"Tweet\", TWEET), (\"Optimistic\", LABEL), (\"Thankful\", LABEL),\n",
        "              (\"Empathetic\", LABEL), (\"Pessimistic\", LABEL), (\"Anxious\", LABEL), (\"Sad\", LABEL),\n",
        "              (\"Annoyed\", LABEL), (\"Denial\", LABEL), (\"Official report\", LABEL),\n",
        "              (\"Surprise\", LABEL), (\"Joking\", LABEL)]\n",
        "\n",
        "train_dataset, test_dataset = torchtext.data.TabularDataset.splits(\n",
        "    path = '/content/google_drive/MyDrive/Colab Notebooks/', train = 'train.csv', test = 'test.csv', format = 'csv', fields = dataFields, skip_header = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04Y8Hg3rnU3N",
        "outputId": "30c494c6-cf90-4602-ad5e-cd5c9fb4c442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples : 8000\n",
            " Number of testing samples : 2000\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training samples : {}\\n Number of testing samples : {}\".format(len(train_dataset), len(test_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FxMFHaR5Bl3"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "TWEET.build_vocab(train_dataset, vectors = 'glove.840B.300d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsAg-FNfHYdX"
      },
      "source": [
        "delete the id and set the label in list"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrhzVetOFMBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_sgcXIp7Odd"
      },
      "source": [
        "# GPU setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY9BocE97UAb",
        "outputId": "834dfa30-bbeb-4a8f-ff63-0621f1f28f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 18 20:09:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8gsgspsMt23"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eKFD8K_Mv5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab5f38a-8ae5-4770-bf90-bd675db49b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.4.0\n",
            "Uninstalling torchtext-0.4.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/torchtext-0.4.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/torchtext/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled torchtext-0.4.0\n"
          ]
        }
      ],
      "source": [
        "pip uninstall torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td6Hl5smqZXh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "03d7bb8e-1526-4670-8914-3da5fadf3047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.2.0\n",
            "  Downloading torchtext-0.2.0-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.2.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.2.0) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.2.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.2.0) (2.10)\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install torchtext==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buDnFn21kxoo"
      },
      "source": [
        "# **Using LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyyvY65EBJI-"
      },
      "source": [
        "**Hyperparameters to be considered**\n",
        "*  Learning Rate\n",
        "*  Hidden Dimension of LSTM\n",
        "*  Dropout Probability\n",
        "*  Threshold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzoXdwNAr4yx"
      },
      "outputs": [],
      "source": [
        "#pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLuNLQg02P0F"
      },
      "outputs": [],
      "source": [
        "\n",
        "vocab = TWEET.vocab\n",
        "BATCH_SIZE = 32\n",
        "n_label = 11\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iter, test_iter = data.BucketIterator.splits(datasets = (train_dataset, test_dataset),\n",
        "                                                   batch_size = BATCH_SIZE,\n",
        "                                                   sort_key = lambda x : len(x.Tweet),\n",
        "                                                   sort_within_batch = False,\n",
        "                                                   repeat = False,\n",
        "                                                   device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsuVaSid0bAb"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1024)\n",
        "for i in range(TWEET.vocab.vectors.shape[0]):\n",
        "    vec = TWEET.vocab.vectors[i]\n",
        "    if torch.sum(vec).item() == 0:\n",
        "        a = np.random.uniform(-0.25, 0.25, 300)\n",
        "        TWEET.vocab.vectors[i] = torch.from_numpy(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGziX-x6cSRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946331cc-7e18-41a2-dc8b-cae793019726"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[torchtext.data.batch.Batch of size 32]\n",
              "\t[.Tweet]:[torch.cuda.LongTensor of size 28x32 (GPU 0)]\n",
              "\t[.Optimistic]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Thankful]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Empathetic]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Pessimistic]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Anxious]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Sad]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Annoyed]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Denial]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Official report]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Surprise]:[torch.cuda.LongTensor of size 32 (GPU 0)]\n",
              "\t[.Joking]:[torch.cuda.LongTensor of size 32 (GPU 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "#batch = next(train_iter.as_numpy_iterator()); batch\n",
        "#batch = next(train_iter.__iter__()); batch\n",
        "q=iter(train_iter)\n",
        "batch = next(q); batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stOmTXDreL0Z"
      },
      "outputs": [],
      "source": [
        "#idea taken from http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
        "class BatchWrapper():\n",
        "    def __init__(self, dl, x_var, y_vars):\n",
        "        self.dl = dl\n",
        "        self.x_var = x_var\n",
        "        self.y_vars = y_vars\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            x = getattr(batch, self.x_var)\n",
        "            if self.y_vars is not None:\n",
        "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim = 1).float()\n",
        "            else:\n",
        "                y = torch.zeros((1))\n",
        "            yield(x, y)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zg5H4OCogXtH"
      },
      "outputs": [],
      "source": [
        "train_dl = BatchWrapper(train_iter, \"Tweet\", ['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', \n",
        "                                              'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'])\n",
        "test_dl = BatchWrapper(test_iter, \"Tweet\", ['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', \n",
        "                                            'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48UVabxn_nSE"
      },
      "outputs": [],
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab, hidden_dim, output_dim, drop_prob, bidirectional = False, use_glove = True):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(len(vocab), embedding_dim)\n",
        "        if use_glove:\n",
        "            self.embeddings.weight.data.copy_(vocab.vectors)\n",
        "            self.embeddings.weight.requires_grad = False\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional = bidirectional, batch_first = True, num_layers = 2)\n",
        "        if bidirectional is True:\n",
        "            self.lin = nn.Linear(2*hidden_dim, 64)\n",
        "        else:\n",
        "            self.lin = nn.Linear(hidden_dim, 64)\n",
        "        self.fc = nn.Linear(64, output_dim)\n",
        "        self.dropout = nn.Dropout(p = drop_prob)\n",
        "    \n",
        "    def forward(self, sentence):\n",
        "        #sentence = [max_len, batch_size]\n",
        "\n",
        "        embed = self.embeddings(torch.transpose(sentence, 0, 1))\n",
        "        #embed = [batch_size, max_len, embedding_dim]\n",
        "        \n",
        "        if self.drop_prob:\n",
        "            embed = self.dropout(embed)\n",
        "        \n",
        "        lstm_out, (hidden, cell) = self.lstm(embed)\n",
        "        #lstm_out = [batch_size, max_len, 2*hidden_dim if bidirectional else hidden_dim]\n",
        "        #hidden = [num_layers, batch_size, hidden_dim]\n",
        "        #cell = [num_layers, batch_size, hidden_dim]\n",
        "        \n",
        "        out = lstm_out[:,-1,:].squeeze()\n",
        "        #out = [batch_size, 2*hidden_dim if bidirectional else hidden_dim]\n",
        "        \n",
        "        out = self.lin(out)\n",
        "        #out = [batch_size, 64]\n",
        "\n",
        "        outputs = self.fc(out)\n",
        "        #outputs = [batch_size, output_dim]\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ifJTTfCIKme"
      },
      "outputs": [],
      "source": [
        "def evaluation_metrics(actual_labels, pred_labels, threshold):\n",
        "    int_pred_labels = pred_labels\n",
        "    for i in range(len(pred_labels)):\n",
        "        for j in range(11):\n",
        "            if int_pred_labels[i][j] >= threshold: int_pred_labels[i][j] = 1\n",
        "            else:\n",
        "                int_pred_labels[i][j] = 0\n",
        "    \n",
        "    ham_loss = hamming_loss(actual_labels, int_pred_labels)\n",
        "    jacc_score = jaccard_score(actual_labels, int_pred_labels, average = 'samples')\n",
        "    lrap = label_ranking_average_precision_score(actual_labels, pred_labels)\n",
        "    f1_macro = f1_score(actual_labels, int_pred_labels, average = 'macro')\n",
        "    f1_micro = f1_score(actual_labels, int_pred_labels, average = 'micro')\n",
        "\n",
        "    return ham_loss, jacc_score, lrap, f1_macro, f1_micro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgZ3c5GvPCZb"
      },
      "outputs": [],
      "source": [
        "def train(model, loss_fn, optimizer, n_epochs, train_dl, threshold):\n",
        "\n",
        "    train_losses = []  \n",
        "    hamming_losses = []\n",
        "    jaccard_scores = []\n",
        "    lraps = []  \n",
        "    iter = 1\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        running_loss = 0.0\n",
        "        pred_labels = []\n",
        "        actual_labels = []\n",
        "        model.train()\n",
        "        for x, y in train_dl:\n",
        "            #print(x.shape, y.shape)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            preds = model(x)\n",
        "\n",
        "            m = nn.Sigmoid()\n",
        "            sig_preds = m(preds)\n",
        "            \n",
        "            for tens in sig_preds:\n",
        "                pred_labels.append(tens.cpu().detach().numpy())\n",
        "            for tens in y:\n",
        "                actual_labels.append(tens.cpu().detach().numpy())\n",
        "\n",
        "            loss = loss_fn(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * x.shape[0]\n",
        "\n",
        "        ham_loss, jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        train_losses.append(epoch_loss)\n",
        "        hamming_losses.append(ham_loss)\n",
        "        lraps.append(lrap)\n",
        "        jaccard_scores.append(jacc_score)\n",
        "        '''\n",
        "        if iter % 5 == 0:\n",
        "            print(\"Epoch: \", epoch)\n",
        "            print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(epoch_loss))\n",
        "            print(\"Hamming Loss : {:.4f}\".format(ham_loss))\n",
        "            print(\"Jaccard Score: {:.4f}\".format(jacc_score))\n",
        "            print(\"Label Ranking Average Precision Score: {:.4f}\".format(lrap))\n",
        "            print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "            print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "            print(\"\\n\")\n",
        "        iter += 1\n",
        "        '''\n",
        "    return train_losses, hamming_losses, jaccard_scores, lraps, f1_macro, f1_micro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJo2hk-oWGVa"
      },
      "outputs": [],
      "source": [
        "def test(model, loss_fn, test_dl, threshold):\n",
        "    running_loss = 0.0\n",
        "    pred_labels = []\n",
        "    actual_labels = []\n",
        "    model.eval()\n",
        "    for x, y in test_dl:\n",
        "        #print(x.shape, y.shape)\n",
        "\n",
        "        preds = model(x)\n",
        "\n",
        "        m = nn.Sigmoid()\n",
        "        sig_preds = m(preds)\n",
        "        \n",
        "        for tens in sig_preds:\n",
        "            pred_labels.append(tens.cpu().detach().numpy())\n",
        "        for tens in y:\n",
        "            actual_labels.append(tens.cpu().detach().numpy())\n",
        "\n",
        "        loss = loss_fn(preds, y)\n",
        "\n",
        "        running_loss += loss.item() * x.shape[0]\n",
        "\n",
        "    ham_loss, jacc_score, lrap, f1_macro, f1_micro = evaluation_metrics(actual_labels, pred_labels, threshold)\n",
        "\n",
        "    test_loss = running_loss / len(test_dataset)\n",
        "    return test_loss, ham_loss, jacc_score, lrap, f1_macro, f1_micro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBWwKr4czLr2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with the dropout or not"
      ],
      "metadata": {
        "id": "WyWCfq1tVvjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [1e-3]\n",
        "hidden_dims = [128, 256]\n",
        "thresholds = [0.4, 0.45, 0.5]\n",
        "dropouts = [True, False]\n",
        "\n",
        "all_models = []\n",
        "iter = 1\n",
        "#number of iterations = 3*2*2 = 12 = number of models\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for threshold in thresholds:\n",
        "            for dropout in dropouts:\n",
        "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = dropout)\n",
        "                model = model.to(device)\n",
        "\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "                loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "                train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "                all_models.append(model)\n",
        "                print(\"Iteration : {}\".format(iter))\n",
        "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout = {}\".format(learning_rate, hidden_dim, threshold, dropout))\n",
        "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "                print(\"\\n\")\n",
        "\n",
        "                test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "                print(\"\\n\")\n",
        "                print(\"----------------------------------------------------------------\")\n",
        "                iter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnGYjLTjVxPN",
        "outputId": "bb804af3-dff2-4d1f-a4bd-87741f4bdc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration : 1\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.4, Dropout = True\n",
            "Binary Cross Entropy With Logits Loss: 0.3947\n",
            "Hamming Loss : 0.1911\n",
            "Jaccard Score: 0.2524\n",
            "Label Ranking Average Precision Score: 0.3902\n",
            "F1 Macro Score: 0.0562\n",
            "F1 Micro Score: 0.2984\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2594\n",
            "Test Hamming Loss : 0.1927\n",
            "Test Jaccard Score: 0.2462\n",
            "Test Label Ranking Average Precision Score: 0.3885\n",
            "Test F1 Macro Score: 0.0561\n",
            "Test F1 Micro Score: 0.2964\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 2\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.4, Dropout = False\n",
            "Binary Cross Entropy With Logits Loss: 0.0266\n",
            "Hamming Loss : 0.0099\n",
            "Jaccard Score: 0.9638\n",
            "Label Ranking Average Precision Score: 0.9676\n",
            "F1 Macro Score: 0.9621\n",
            "F1 Micro Score: 0.9717\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.7199\n",
            "Test Hamming Loss : 0.2278\n",
            "Test Jaccard Score: 0.3269\n",
            "Test Label Ranking Average Precision Score: 0.4076\n",
            "Test F1 Macro Score: 0.3758\n",
            "Test F1 Micro Score: 0.4034\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 3\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.45, Dropout = True\n",
            "Binary Cross Entropy With Logits Loss: 0.3948\n",
            "Hamming Loss : 0.1842\n",
            "Jaccard Score: 0.1464\n",
            "Label Ranking Average Precision Score: 0.3044\n",
            "F1 Macro Score: 0.0393\n",
            "F1 Micro Score: 0.1557\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2593\n",
            "Test Hamming Loss : 0.1927\n",
            "Test Jaccard Score: 0.2462\n",
            "Test Label Ranking Average Precision Score: 0.3885\n",
            "Test F1 Macro Score: 0.0561\n",
            "Test F1 Micro Score: 0.2964\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 4\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.45, Dropout = False\n",
            "Binary Cross Entropy With Logits Loss: 0.0189\n",
            "Hamming Loss : 0.0070\n",
            "Jaccard Score: 0.9736\n",
            "Label Ranking Average Precision Score: 0.9766\n",
            "F1 Macro Score: 0.9735\n",
            "F1 Micro Score: 0.9788\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.7700\n",
            "Test Hamming Loss : 0.2268\n",
            "Test Jaccard Score: 0.3297\n",
            "Test Label Ranking Average Precision Score: 0.4058\n",
            "Test F1 Macro Score: 0.3935\n",
            "Test F1 Micro Score: 0.4127\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 5\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = True\n",
            "Binary Cross Entropy With Logits Loss: 0.3953\n",
            "Hamming Loss : 0.1820\n",
            "Jaccard Score: 0.0321\n",
            "Label Ranking Average Precision Score: 0.2065\n",
            "F1 Macro Score: 0.0000\n",
            "F1 Micro Score: 0.0000\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2596\n",
            "Test Hamming Loss : 0.1830\n",
            "Test Jaccard Score: 0.0000\n",
            "Test Label Ranking Average Precision Score: 0.1830\n",
            "Test F1 Macro Score: 0.0000\n",
            "Test F1 Micro Score: 0.0000\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 6\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout = False\n",
            "Binary Cross Entropy With Logits Loss: 0.0275\n",
            "Hamming Loss : 0.0102\n",
            "Jaccard Score: 0.9623\n",
            "Label Ranking Average Precision Score: 0.9675\n",
            "F1 Macro Score: 0.9620\n",
            "F1 Micro Score: 0.9709\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.7788\n",
            "Test Hamming Loss : 0.2164\n",
            "Test Jaccard Score: 0.3130\n",
            "Test Label Ranking Average Precision Score: 0.4034\n",
            "Test F1 Macro Score: 0.3540\n",
            "Test F1 Micro Score: 0.3927\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 7\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.4, Dropout = True\n",
            "Binary Cross Entropy With Logits Loss: 0.3941\n",
            "Hamming Loss : 0.1912\n",
            "Jaccard Score: 0.2510\n",
            "Label Ranking Average Precision Score: 0.3902\n",
            "F1 Macro Score: 0.0562\n",
            "F1 Micro Score: 0.2984\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2597\n",
            "Test Hamming Loss : 0.1927\n",
            "Test Jaccard Score: 0.2462\n",
            "Test Label Ranking Average Precision Score: 0.3885\n",
            "Test F1 Macro Score: 0.0561\n",
            "Test F1 Micro Score: 0.2964\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 8\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.4, Dropout = False\n",
            "Binary Cross Entropy With Logits Loss: 0.0152\n",
            "Hamming Loss : 0.0055\n",
            "Jaccard Score: 0.9802\n",
            "Label Ranking Average Precision Score: 0.9822\n",
            "F1 Macro Score: 0.9820\n",
            "F1 Micro Score: 0.9848\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.7323\n",
            "Test Hamming Loss : 0.2235\n",
            "Test Jaccard Score: 0.3224\n",
            "Test Label Ranking Average Precision Score: 0.4027\n",
            "Test F1 Macro Score: 0.3822\n",
            "Test F1 Micro Score: 0.4075\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 9\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.45, Dropout = True\n",
            "Binary Cross Entropy With Logits Loss: 0.3944\n",
            "Hamming Loss : 0.1842\n",
            "Jaccard Score: 0.1777\n",
            "Label Ranking Average Precision Score: 0.3305\n",
            "F1 Macro Score: 0.0429\n",
            "F1 Micro Score: 0.1797\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2594\n",
            "Test Hamming Loss : 0.1830\n",
            "Test Jaccard Score: 0.0000\n",
            "Test Label Ranking Average Precision Score: 0.1830\n",
            "Test F1 Macro Score: 0.0000\n",
            "Test F1 Micro Score: 0.0000\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 10\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.45, Dropout = False\n",
            "Binary Cross Entropy With Logits Loss: 0.0137\n",
            "Hamming Loss : 0.0050\n",
            "Jaccard Score: 0.9813\n",
            "Label Ranking Average Precision Score: 0.9833\n",
            "F1 Macro Score: 0.9827\n",
            "F1 Micro Score: 0.9862\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.8412\n",
            "Test Hamming Loss : 0.2118\n",
            "Test Jaccard Score: 0.3278\n",
            "Test Label Ranking Average Precision Score: 0.4139\n",
            "Test F1 Macro Score: 0.3583\n",
            "Test F1 Micro Score: 0.4082\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 11\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.5, Dropout = True\n",
            "Binary Cross Entropy With Logits Loss: 0.3946\n",
            "Hamming Loss : 0.1820\n",
            "Jaccard Score: 0.0382\n",
            "Label Ranking Average Precision Score: 0.2126\n",
            "F1 Macro Score: 0.0000\n",
            "F1 Micro Score: 0.0000\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2594\n",
            "Test Hamming Loss : 0.1830\n",
            "Test Jaccard Score: 0.0000\n",
            "Test Label Ranking Average Precision Score: 0.1830\n",
            "Test F1 Macro Score: 0.0000\n",
            "Test F1 Micro Score: 0.0000\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 12\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.5, Dropout = False\n",
            "Binary Cross Entropy With Logits Loss: 0.0129\n",
            "Hamming Loss : 0.0047\n",
            "Jaccard Score: 0.9823\n",
            "Label Ranking Average Precision Score: 0.9844\n",
            "F1 Macro Score: 0.9846\n",
            "F1 Micro Score: 0.9864\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.7960\n",
            "Test Hamming Loss : 0.2132\n",
            "Test Jaccard Score: 0.3300\n",
            "Test Label Ranking Average Precision Score: 0.4146\n",
            "Test F1 Macro Score: 0.3651\n",
            "Test F1 Micro Score: 0.4120\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arCjOqoikmhG"
      },
      "source": [
        "# Using the different parpameters hidden, threshold, dropout\n",
        "*  it necessary using the dropout\n",
        "*  the values of the dropout probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9jAW-ZjkpP_",
        "outputId": "d0c842ff-8a19-41a6-888b-c19ce47f624b"
      },
      "source": [
        "learning_rates = [1e-3]\n",
        "hidden_dims = [128, 256]\n",
        "thresholds = [0.4, 0.45, 0.5]\n",
        "drop_probs = [0.4, 0.5, 0.6, 0.7]\n",
        "\n",
        "all_models = []\n",
        "iter = 1\n",
        "#number of iterations = 4*3*2 = 24 = number of models\n",
        "\n",
        "for learning_rate in learning_rates:\n",
        "    for hidden_dim in hidden_dims:\n",
        "        for threshold in thresholds:\n",
        "            for drop_prob in drop_probs:\n",
        "                model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "                model = model.to(device)\n",
        "\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "                loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "                train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "                all_models.append(model)\n",
        "                print(\"Iteration : {}\".format(iter))\n",
        "                print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout Probability = {}\".format(learning_rate, hidden_dim, threshold, drop_prob))\n",
        "                print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "                print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "                print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "                print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "                print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "                print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "                print(\"\\n\")\n",
        "\n",
        "                test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "                print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "                print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "                print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "                print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "                print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "                print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "                print(\"\\n\")\n",
        "                print(\"----------------------------------------------------------------\")\n",
        "                iter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration : 1\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.4, Dropout Probability = 0.4\n",
            "Binary Cross Entropy With Logits Loss: 0.1636\n",
            "Hamming Loss : 0.0738\n",
            "Jaccard Score: 0.7270\n",
            "Label Ranking Average Precision Score: 0.7573\n",
            "F1 Macro Score: 0.7567\n",
            "F1 Micro Score: 0.7934\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3464\n",
            "Test Hamming Loss : 0.1975\n",
            "Test Jaccard Score: 0.3715\n",
            "Test Label Ranking Average Precision Score: 0.4452\n",
            "Test F1 Macro Score: 0.4219\n",
            "Test F1 Micro Score: 0.4610\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 2\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.4, Dropout Probability = 0.5\n",
            "Binary Cross Entropy With Logits Loss: 0.1982\n",
            "Hamming Loss : 0.0904\n",
            "Jaccard Score: 0.6699\n",
            "Label Ranking Average Precision Score: 0.7070\n",
            "F1 Macro Score: 0.6892\n",
            "F1 Micro Score: 0.7444\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2983\n",
            "Test Hamming Loss : 0.1900\n",
            "Test Jaccard Score: 0.3818\n",
            "Test Label Ranking Average Precision Score: 0.4573\n",
            "Test F1 Macro Score: 0.4214\n",
            "Test F1 Micro Score: 0.4664\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 3\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.4, Dropout Probability = 0.6\n",
            "Binary Cross Entropy With Logits Loss: 0.2301\n",
            "Hamming Loss : 0.1073\n",
            "Jaccard Score: 0.6106\n",
            "Label Ranking Average Precision Score: 0.6570\n",
            "F1 Macro Score: 0.6288\n",
            "F1 Micro Score: 0.6908\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2579\n",
            "Test Hamming Loss : 0.1760\n",
            "Test Jaccard Score: 0.4011\n",
            "Test Label Ranking Average Precision Score: 0.4789\n",
            "Test F1 Macro Score: 0.4215\n",
            "Test F1 Micro Score: 0.4881\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 4\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.4, Dropout Probability = 0.7\n",
            "Binary Cross Entropy With Logits Loss: 0.2766\n",
            "Hamming Loss : 0.1303\n",
            "Jaccard Score: 0.5248\n",
            "Label Ranking Average Precision Score: 0.5867\n",
            "F1 Macro Score: 0.5158\n",
            "F1 Micro Score: 0.6106\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2430\n",
            "Test Hamming Loss : 0.1763\n",
            "Test Jaccard Score: 0.3963\n",
            "Test Label Ranking Average Precision Score: 0.4777\n",
            "Test F1 Macro Score: 0.4111\n",
            "Test F1 Micro Score: 0.4791\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 5\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.45, Dropout Probability = 0.4\n",
            "Binary Cross Entropy With Logits Loss: 0.1517\n",
            "Hamming Loss : 0.0669\n",
            "Jaccard Score: 0.7465\n",
            "Label Ranking Average Precision Score: 0.7785\n",
            "F1 Macro Score: 0.7674\n",
            "F1 Micro Score: 0.8094\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3391\n",
            "Test Hamming Loss : 0.1977\n",
            "Test Jaccard Score: 0.3597\n",
            "Test Label Ranking Average Precision Score: 0.4410\n",
            "Test F1 Macro Score: 0.4033\n",
            "Test F1 Micro Score: 0.4426\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 6\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.45, Dropout Probability = 0.5\n",
            "Binary Cross Entropy With Logits Loss: 0.1834\n",
            "Hamming Loss : 0.0818\n",
            "Jaccard Score: 0.6910\n",
            "Label Ranking Average Precision Score: 0.7316\n",
            "F1 Macro Score: 0.7079\n",
            "F1 Micro Score: 0.7623\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3135\n",
            "Test Hamming Loss : 0.1812\n",
            "Test Jaccard Score: 0.3639\n",
            "Test Label Ranking Average Precision Score: 0.4544\n",
            "Test F1 Macro Score: 0.4108\n",
            "Test F1 Micro Score: 0.4514\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 7\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.45, Dropout Probability = 0.6\n",
            "Binary Cross Entropy With Logits Loss: 0.2459\n",
            "Hamming Loss : 0.1135\n",
            "Jaccard Score: 0.5730\n",
            "Label Ranking Average Precision Score: 0.6347\n",
            "F1 Macro Score: 0.5773\n",
            "F1 Micro Score: 0.6521\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2829\n",
            "Test Hamming Loss : 0.1824\n",
            "Test Jaccard Score: 0.3586\n",
            "Test Label Ranking Average Precision Score: 0.4538\n",
            "Test F1 Macro Score: 0.3863\n",
            "Test F1 Micro Score: 0.4377\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 8\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.45, Dropout Probability = 0.7\n",
            "Binary Cross Entropy With Logits Loss: 0.2750\n",
            "Hamming Loss : 0.1272\n",
            "Jaccard Score: 0.5139\n",
            "Label Ranking Average Precision Score: 0.5884\n",
            "F1 Macro Score: 0.5098\n",
            "F1 Micro Score: 0.5971\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2377\n",
            "Test Hamming Loss : 0.1664\n",
            "Test Jaccard Score: 0.3995\n",
            "Test Label Ranking Average Precision Score: 0.4890\n",
            "Test F1 Macro Score: 0.4203\n",
            "Test F1 Micro Score: 0.4797\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 9\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout Probability = 0.4\n",
            "Binary Cross Entropy With Logits Loss: 0.1496\n",
            "Hamming Loss : 0.0650\n",
            "Jaccard Score: 0.7490\n",
            "Label Ranking Average Precision Score: 0.7852\n",
            "F1 Macro Score: 0.7665\n",
            "F1 Micro Score: 0.8102\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3292\n",
            "Test Hamming Loss : 0.1795\n",
            "Test Jaccard Score: 0.3778\n",
            "Test Label Ranking Average Precision Score: 0.4644\n",
            "Test F1 Macro Score: 0.4020\n",
            "Test F1 Micro Score: 0.4619\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 10\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout Probability = 0.5\n",
            "Binary Cross Entropy With Logits Loss: 0.1872\n",
            "Hamming Loss : 0.0840\n",
            "Jaccard Score: 0.6732\n",
            "Label Ranking Average Precision Score: 0.7218\n",
            "F1 Macro Score: 0.6944\n",
            "F1 Micro Score: 0.7466\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3242\n",
            "Test Hamming Loss : 0.1855\n",
            "Test Jaccard Score: 0.3622\n",
            "Test Label Ranking Average Precision Score: 0.4521\n",
            "Test F1 Macro Score: 0.3875\n",
            "Test F1 Micro Score: 0.4431\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 11\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout Probability = 0.6\n",
            "Binary Cross Entropy With Logits Loss: 0.2438\n",
            "Hamming Loss : 0.1100\n",
            "Jaccard Score: 0.5696\n",
            "Label Ranking Average Precision Score: 0.6402\n",
            "F1 Macro Score: 0.5640\n",
            "F1 Micro Score: 0.6476\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2800\n",
            "Test Hamming Loss : 0.1754\n",
            "Test Jaccard Score: 0.3472\n",
            "Test Label Ranking Average Precision Score: 0.4519\n",
            "Test F1 Macro Score: 0.3710\n",
            "Test F1 Micro Score: 0.4270\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 12\n",
            "Learning Rate : 0.001, Hidden Dimension : 128, Threshold : 0.5, Dropout Probability = 0.7\n",
            "Binary Cross Entropy With Logits Loss: 0.2721\n",
            "Hamming Loss : 0.1250\n",
            "Jaccard Score: 0.5025\n",
            "Label Ranking Average Precision Score: 0.5879\n",
            "F1 Macro Score: 0.4873\n",
            "F1 Micro Score: 0.5832\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2362\n",
            "Test Hamming Loss : 0.1624\n",
            "Test Jaccard Score: 0.3862\n",
            "Test Label Ranking Average Precision Score: 0.4877\n",
            "Test F1 Macro Score: 0.3910\n",
            "Test F1 Micro Score: 0.4610\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 13\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.4, Dropout Probability = 0.4\n",
            "Binary Cross Entropy With Logits Loss: 0.0694\n",
            "Hamming Loss : 0.0288\n",
            "Jaccard Score: 0.8886\n",
            "Label Ranking Average Precision Score: 0.8995\n",
            "F1 Macro Score: 0.9100\n",
            "F1 Micro Score: 0.9214\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.5045\n",
            "Test Hamming Loss : 0.2067\n",
            "Test Jaccard Score: 0.3550\n",
            "Test Label Ranking Average Precision Score: 0.4297\n",
            "Test F1 Macro Score: 0.4188\n",
            "Test F1 Micro Score: 0.4467\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 14\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.4, Dropout Probability = 0.5\n",
            "Binary Cross Entropy With Logits Loss: 0.1034\n",
            "Hamming Loss : 0.0451\n",
            "Jaccard Score: 0.8298\n",
            "Label Ranking Average Precision Score: 0.8464\n",
            "F1 Macro Score: 0.8624\n",
            "F1 Micro Score: 0.8770\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.4134\n",
            "Test Hamming Loss : 0.2034\n",
            "Test Jaccard Score: 0.3717\n",
            "Test Label Ranking Average Precision Score: 0.4413\n",
            "Test F1 Macro Score: 0.4207\n",
            "Test F1 Micro Score: 0.4638\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 15\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.4, Dropout Probability = 0.6\n",
            "Binary Cross Entropy With Logits Loss: 0.1852\n",
            "Hamming Loss : 0.0841\n",
            "Jaccard Score: 0.6915\n",
            "Label Ranking Average Precision Score: 0.7253\n",
            "F1 Macro Score: 0.7292\n",
            "F1 Micro Score: 0.7642\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3290\n",
            "Test Hamming Loss : 0.1973\n",
            "Test Jaccard Score: 0.3767\n",
            "Test Label Ranking Average Precision Score: 0.4464\n",
            "Test F1 Macro Score: 0.4082\n",
            "Test F1 Micro Score: 0.4663\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 16\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.4, Dropout Probability = 0.7\n",
            "Binary Cross Entropy With Logits Loss: 0.2402\n",
            "Hamming Loss : 0.1119\n",
            "Jaccard Score: 0.5949\n",
            "Label Ranking Average Precision Score: 0.6437\n",
            "F1 Macro Score: 0.6096\n",
            "F1 Micro Score: 0.6742\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2773\n",
            "Test Hamming Loss : 0.1837\n",
            "Test Jaccard Score: 0.3778\n",
            "Test Label Ranking Average Precision Score: 0.4626\n",
            "Test F1 Macro Score: 0.4074\n",
            "Test F1 Micro Score: 0.4584\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 17\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.45, Dropout Probability = 0.4\n",
            "Binary Cross Entropy With Logits Loss: 0.0637\n",
            "Hamming Loss : 0.0260\n",
            "Jaccard Score: 0.8978\n",
            "Label Ranking Average Precision Score: 0.9091\n",
            "F1 Macro Score: 0.9186\n",
            "F1 Micro Score: 0.9284\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.4955\n",
            "Test Hamming Loss : 0.2021\n",
            "Test Jaccard Score: 0.3530\n",
            "Test Label Ranking Average Precision Score: 0.4348\n",
            "Test F1 Macro Score: 0.3966\n",
            "Test F1 Micro Score: 0.4369\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 18\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.45, Dropout Probability = 0.5\n",
            "Binary Cross Entropy With Logits Loss: 0.1249\n",
            "Hamming Loss : 0.0539\n",
            "Jaccard Score: 0.7942\n",
            "Label Ranking Average Precision Score: 0.8185\n",
            "F1 Macro Score: 0.8264\n",
            "F1 Micro Score: 0.8490\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3757\n",
            "Test Hamming Loss : 0.1885\n",
            "Test Jaccard Score: 0.3660\n",
            "Test Label Ranking Average Precision Score: 0.4493\n",
            "Test F1 Macro Score: 0.4178\n",
            "Test F1 Micro Score: 0.4561\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 19\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.45, Dropout Probability = 0.6\n",
            "Binary Cross Entropy With Logits Loss: 0.1897\n",
            "Hamming Loss : 0.0837\n",
            "Jaccard Score: 0.6857\n",
            "Label Ranking Average Precision Score: 0.7273\n",
            "F1 Macro Score: 0.7018\n",
            "F1 Micro Score: 0.7550\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3196\n",
            "Test Hamming Loss : 0.1877\n",
            "Test Jaccard Score: 0.3859\n",
            "Test Label Ranking Average Precision Score: 0.4657\n",
            "Test F1 Macro Score: 0.4133\n",
            "Test F1 Micro Score: 0.4659\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 20\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.45, Dropout Probability = 0.7\n",
            "Binary Cross Entropy With Logits Loss: 0.2381\n",
            "Hamming Loss : 0.1081\n",
            "Jaccard Score: 0.5901\n",
            "Label Ranking Average Precision Score: 0.6492\n",
            "F1 Macro Score: 0.6033\n",
            "F1 Micro Score: 0.6698\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2943\n",
            "Test Hamming Loss : 0.1889\n",
            "Test Jaccard Score: 0.3723\n",
            "Test Label Ranking Average Precision Score: 0.4541\n",
            "Test F1 Macro Score: 0.4112\n",
            "Test F1 Micro Score: 0.4555\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 21\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.5, Dropout Probability = 0.4\n",
            "Binary Cross Entropy With Logits Loss: 0.0934\n",
            "Hamming Loss : 0.0386\n",
            "Jaccard Score: 0.8503\n",
            "Label Ranking Average Precision Score: 0.8704\n",
            "F1 Macro Score: 0.8740\n",
            "F1 Micro Score: 0.8914\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.4509\n",
            "Test Hamming Loss : 0.1961\n",
            "Test Jaccard Score: 0.3560\n",
            "Test Label Ranking Average Precision Score: 0.4398\n",
            "Test F1 Macro Score: 0.4030\n",
            "Test F1 Micro Score: 0.4423\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 22\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.5, Dropout Probability = 0.5\n",
            "Binary Cross Entropy With Logits Loss: 0.1212\n",
            "Hamming Loss : 0.0521\n",
            "Jaccard Score: 0.7986\n",
            "Label Ranking Average Precision Score: 0.8264\n",
            "F1 Macro Score: 0.8251\n",
            "F1 Micro Score: 0.8512\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3908\n",
            "Test Hamming Loss : 0.1908\n",
            "Test Jaccard Score: 0.3562\n",
            "Test Label Ranking Average Precision Score: 0.4424\n",
            "Test F1 Macro Score: 0.3967\n",
            "Test F1 Micro Score: 0.4405\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 23\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.5, Dropout Probability = 0.6\n",
            "Binary Cross Entropy With Logits Loss: 0.1711\n",
            "Hamming Loss : 0.0751\n",
            "Jaccard Score: 0.7081\n",
            "Label Ranking Average Precision Score: 0.7498\n",
            "F1 Macro Score: 0.7390\n",
            "F1 Micro Score: 0.7780\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.3383\n",
            "Test Hamming Loss : 0.1776\n",
            "Test Jaccard Score: 0.3768\n",
            "Test Label Ranking Average Precision Score: 0.4650\n",
            "Test F1 Macro Score: 0.3990\n",
            "Test F1 Micro Score: 0.4603\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n",
            "Iteration : 24\n",
            "Learning Rate : 0.001, Hidden Dimension : 256, Threshold : 0.5, Dropout Probability = 0.7\n",
            "Binary Cross Entropy With Logits Loss: 0.2476\n",
            "Hamming Loss : 0.1133\n",
            "Jaccard Score: 0.5542\n",
            "Label Ranking Average Precision Score: 0.6281\n",
            "F1 Macro Score: 0.5618\n",
            "F1 Micro Score: 0.6338\n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2707\n",
            "Test Hamming Loss : 0.1732\n",
            "Test Jaccard Score: 0.3734\n",
            "Test Label Ranking Average Precision Score: 0.4715\n",
            "Test F1 Macro Score: 0.3720\n",
            "Test F1 Micro Score: 0.4481\n",
            "\n",
            "\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSBb2r1YJqrq"
      },
      "source": [
        "# Final evaluation using narrowed down hyperparameters drop_prob = 0.65 \n",
        "## hidden_dim = 128\n",
        "## threshold = 0.5\n",
        "## drop_prob = 0.65\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y46-8KJdqHvl",
        "outputId": "37e79bdf-a35b-4c4e-fcb0-5527545e8366"
      },
      "source": [
        "%%time\n",
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.65\n",
        "\n",
        "all_bce_losses = []\n",
        "all_hamm_losses = []\n",
        "all_jacc_scores = []\n",
        "all_lraps = []\n",
        "all_f1_macro = []\n",
        "all_f1_micro = []\n",
        "all_models = []\n",
        "\n",
        "for exp in range(1, 11):\n",
        "\n",
        "    print(\"Experiment {}\".format(exp), '\\n\\n')\n",
        "\n",
        "    model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "    torch.save(model, 'model{}.pth'.format(exp))\n",
        "    '''\n",
        "    print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout Probability = {}\".format(learning_rate, hidden_dim, threshold, drop_prob))\n",
        "    print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "    print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "    print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "    print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "    print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "    print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "\n",
        "    test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "    all_bce_losses.append(test_loss)\n",
        "    all_hamm_losses.append(test_hamm_loss)\n",
        "    all_jacc_scores.append(test_jacc_score)\n",
        "    all_lraps.append(test_lrap)\n",
        "    all_f1_macro.append(test_f1_macro)\n",
        "    all_f1_micro.append(test_f1_micro)\n",
        "\n",
        "\n",
        "    print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "    print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "    print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "    print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "    print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "    print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 1 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2609\n",
            "Test Hamming Loss : 0.1835\n",
            "Test Jaccard Score: 0.3640\n",
            "Test Label Ranking Average Precision Score: 0.4586\n",
            "Test F1 Macro Score: 0.3942\n",
            "Test F1 Micro Score: 0.4376\n",
            "\n",
            "\n",
            "Experiment 2 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2584\n",
            "Test Hamming Loss : 0.1675\n",
            "Test Jaccard Score: 0.3823\n",
            "Test Label Ranking Average Precision Score: 0.4821\n",
            "Test F1 Macro Score: 0.3905\n",
            "Test F1 Micro Score: 0.4554\n",
            "\n",
            "\n",
            "Experiment 3 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2445\n",
            "Test Hamming Loss : 0.1628\n",
            "Test Jaccard Score: 0.4003\n",
            "Test Label Ranking Average Precision Score: 0.4960\n",
            "Test F1 Macro Score: 0.4025\n",
            "Test F1 Micro Score: 0.4749\n",
            "\n",
            "\n",
            "Experiment 4 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2569\n",
            "Test Hamming Loss : 0.1683\n",
            "Test Jaccard Score: 0.3893\n",
            "Test Label Ranking Average Precision Score: 0.4856\n",
            "Test F1 Macro Score: 0.4091\n",
            "Test F1 Micro Score: 0.4651\n",
            "\n",
            "\n",
            "Experiment 5 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2456\n",
            "Test Hamming Loss : 0.1654\n",
            "Test Jaccard Score: 0.3876\n",
            "Test Label Ranking Average Precision Score: 0.4848\n",
            "Test F1 Macro Score: 0.4112\n",
            "Test F1 Micro Score: 0.4639\n",
            "\n",
            "\n",
            "Experiment 6 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2534\n",
            "Test Hamming Loss : 0.1691\n",
            "Test Jaccard Score: 0.3958\n",
            "Test Label Ranking Average Precision Score: 0.4895\n",
            "Test F1 Macro Score: 0.4086\n",
            "Test F1 Micro Score: 0.4675\n",
            "\n",
            "\n",
            "Experiment 7 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2488\n",
            "Test Hamming Loss : 0.1660\n",
            "Test Jaccard Score: 0.3695\n",
            "Test Label Ranking Average Precision Score: 0.4734\n",
            "Test F1 Macro Score: 0.3917\n",
            "Test F1 Micro Score: 0.4475\n",
            "\n",
            "\n",
            "Experiment 8 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2493\n",
            "Test Hamming Loss : 0.1659\n",
            "Test Jaccard Score: 0.3826\n",
            "Test Label Ranking Average Precision Score: 0.4835\n",
            "Test F1 Macro Score: 0.3995\n",
            "Test F1 Micro Score: 0.4565\n",
            "\n",
            "\n",
            "Experiment 9 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2460\n",
            "Test Hamming Loss : 0.1634\n",
            "Test Jaccard Score: 0.3885\n",
            "Test Label Ranking Average Precision Score: 0.4893\n",
            "Test F1 Macro Score: 0.3931\n",
            "Test F1 Micro Score: 0.4611\n",
            "\n",
            "\n",
            "Experiment 10 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2478\n",
            "Test Hamming Loss : 0.1634\n",
            "Test Jaccard Score: 0.3942\n",
            "Test Label Ranking Average Precision Score: 0.4890\n",
            "Test F1 Macro Score: 0.4132\n",
            "Test F1 Micro Score: 0.4759\n",
            "\n",
            "\n",
            "CPU times: user 21min 28s, sys: 20.8 s, total: 21min 48s\n",
            "Wall time: 21min 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB4ms2mOp8Ee",
        "outputId": "90eda759-c96e-415a-e625-e31bfc4f8527"
      },
      "source": [
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_hamm_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_lraps)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_f1_macro)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_f1_micro)/10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Binary Cross Entropy With Logits Loss: 0.2512\n",
            "Average Hamming Loss : 0.1675\n",
            "Average Jaccard Score: 0.3854\n",
            "Average Label Ranking Average Precision Score: 0.4832\n",
            "Average F1 Macro Score: 0.4013\n",
            "Average F1 Micro Score: 0.4605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWZ735YQBrzC"
      },
      "source": [
        "# Using a different value of dropout probability drop_prob = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAZtbnqJB4aX",
        "outputId": "9be7c997-e4ac-45d8-c271-d0dfadeca560"
      },
      "source": [
        "%%time\n",
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.6\n",
        "\n",
        "all_bce_losses = []\n",
        "all_hamm_losses = []\n",
        "all_jacc_scores = []\n",
        "all_lraps = []\n",
        "all_f1_macro = []\n",
        "all_f1_micro = []\n",
        "all_models = []\n",
        "\n",
        "for exp in range(1, 11):\n",
        "\n",
        "    print(\"Experiment {}\".format(exp), '\\n\\n')\n",
        "\n",
        "    model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "    torch.save(model, 'model{}.pth'.format(exp))\n",
        "    '''\n",
        "    print(\"Learning Rate : {}, Hidden Dimension : {}, Threshold : {}, Dropout Probability = {}\".format(learning_rate, hidden_dim, threshold, drop_prob))\n",
        "    print(\"Binary Cross Entropy With Logits Loss: {:.4f}\".format(min(train_loss)))\n",
        "    print(\"Hamming Loss : {:.4f}\".format(min(hamm_loss)))\n",
        "    print(\"Jaccard Score: {:.4f}\".format(max(jacc_score)))\n",
        "    print(\"Label Ranking Average Precision Score: {:.4f}\".format(max(lrap)))\n",
        "    print(\"F1 Macro Score: {:.4f}\".format(f1_macro))\n",
        "    print(\"F1 Micro Score: {:.4f}\".format(f1_micro))\n",
        "    print(\"\\n\")\n",
        "    '''\n",
        "\n",
        "    test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "    all_bce_losses.append(test_loss)\n",
        "    all_hamm_losses.append(test_hamm_loss)\n",
        "    all_jacc_scores.append(test_jacc_score)\n",
        "    all_lraps.append(test_lrap)\n",
        "    all_f1_macro.append(test_f1_macro)\n",
        "    all_f1_micro.append(test_f1_micro)\n",
        "\n",
        "\n",
        "    print(\"TestBinary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "    print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "    print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "    print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "    print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "    print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 1 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2748\n",
            "Test Hamming Loss : 0.1748\n",
            "Test Jaccard Score: 0.3879\n",
            "Test Label Ranking Average Precision Score: 0.4767\n",
            "Test F1 Macro Score: 0.4095\n",
            "Test F1 Micro Score: 0.4663\n",
            "\n",
            "\n",
            "Experiment 2 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2572\n",
            "Test Hamming Loss : 0.1656\n",
            "Test Jaccard Score: 0.3887\n",
            "Test Label Ranking Average Precision Score: 0.4859\n",
            "Test F1 Macro Score: 0.4266\n",
            "Test F1 Micro Score: 0.4675\n",
            "\n",
            "\n",
            "Experiment 3 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2660\n",
            "Test Hamming Loss : 0.1747\n",
            "Test Jaccard Score: 0.3684\n",
            "Test Label Ranking Average Precision Score: 0.4649\n",
            "Test F1 Macro Score: 0.3970\n",
            "Test F1 Micro Score: 0.4474\n",
            "\n",
            "\n",
            "Experiment 4 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2640\n",
            "Test Hamming Loss : 0.1686\n",
            "Test Jaccard Score: 0.3877\n",
            "Test Label Ranking Average Precision Score: 0.4831\n",
            "Test F1 Macro Score: 0.4055\n",
            "Test F1 Micro Score: 0.4629\n",
            "\n",
            "\n",
            "Experiment 5 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2578\n",
            "Test Hamming Loss : 0.1672\n",
            "Test Jaccard Score: 0.3860\n",
            "Test Label Ranking Average Precision Score: 0.4826\n",
            "Test F1 Macro Score: 0.4107\n",
            "Test F1 Micro Score: 0.4648\n",
            "\n",
            "\n",
            "Experiment 6 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2649\n",
            "Test Hamming Loss : 0.1701\n",
            "Test Jaccard Score: 0.3847\n",
            "Test Label Ranking Average Precision Score: 0.4807\n",
            "Test F1 Macro Score: 0.3873\n",
            "Test F1 Micro Score: 0.4581\n",
            "\n",
            "\n",
            "Experiment 7 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2637\n",
            "Test Hamming Loss : 0.1682\n",
            "Test Jaccard Score: 0.3816\n",
            "Test Label Ranking Average Precision Score: 0.4778\n",
            "Test F1 Macro Score: 0.4111\n",
            "Test F1 Micro Score: 0.4611\n",
            "\n",
            "\n",
            "Experiment 8 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2694\n",
            "Test Hamming Loss : 0.1702\n",
            "Test Jaccard Score: 0.3682\n",
            "Test Label Ranking Average Precision Score: 0.4676\n",
            "Test F1 Macro Score: 0.4066\n",
            "Test F1 Micro Score: 0.4503\n",
            "\n",
            "\n",
            "Experiment 9 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2707\n",
            "Test Hamming Loss : 0.1702\n",
            "Test Jaccard Score: 0.3795\n",
            "Test Label Ranking Average Precision Score: 0.4786\n",
            "Test F1 Macro Score: 0.3925\n",
            "Test F1 Micro Score: 0.4535\n",
            "\n",
            "\n",
            "Experiment 10 \n",
            "\n",
            "\n",
            "TestBinary Cross Entropy With Logits Loss: 0.2712\n",
            "Test Hamming Loss : 0.1758\n",
            "Test Jaccard Score: 0.3672\n",
            "Test Label Ranking Average Precision Score: 0.4637\n",
            "Test F1 Macro Score: 0.4010\n",
            "Test F1 Micro Score: 0.4465\n",
            "\n",
            "\n",
            "CPU times: user 21min 9s, sys: 10.6 s, total: 21min 20s\n",
            "Wall time: 21min 12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yTHqv6lCKUW",
        "outputId": "0cd4c381-e5a8-4b6a-a16c-7eb7a5f911bd"
      },
      "source": [
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_hamm_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_lraps)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_f1_macro)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_f1_micro)/10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Binary Cross Entropy With Logits Loss: 0.2660\n",
            "Average Hamming Loss : 0.1705\n",
            "Average Jaccard Score: 0.3800\n",
            "Average Label Ranking Average Precision Score: 0.4761\n",
            "Average F1 Macro Score: 0.4048\n",
            "Average F1 Micro Score: 0.4579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvm9AvjUdDl5"
      },
      "source": [
        "# Comparison between having GloVe Vectors and not having GloVe Vectors \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwd0zhrTyM0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d483c756-49e0-4688-ab43-0c0cc5b81e12"
      },
      "source": [
        "%%time\n",
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.65\n",
        "\n",
        "all_glove_losses = []\n",
        "all_glove_ham = []\n",
        "all_glove_jacc = []\n",
        "all_glove_lrap = []\n",
        "all_glove_f1mac = []\n",
        "all_glove_f1mic = []\n",
        "\n",
        "all_nonglove_losses = []\n",
        "all_nonglove_ham = []\n",
        "all_nonglove_jacc = []\n",
        "all_nonglove_lrap = []\n",
        "all_nonglove_f1mac = []\n",
        "all_nonglove_f1mic = []\n",
        "\n",
        "glove = [True, False]\n",
        "for exp in range(1, 11):\n",
        "    for g in glove:\n",
        "        model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob, use_glove = g)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "        test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "        print(\"Test Binary Cross Entropy With Logits Loss: {:.4f}\".format(test_loss))\n",
        "        print(\"Test Hamming Loss : {:.4f}\".format(test_hamm_loss))\n",
        "        print(\"Test Jaccard Score: {:.4f}\".format(test_jacc_score))\n",
        "        print(\"Test Label Ranking Average Precision Score: {:.4f}\".format(test_lrap))\n",
        "        print(\"Test F1 Macro Score: {:.4f}\".format(test_f1_macro))\n",
        "        print(\"Test F1 Micro Score: {:.4f}\".format(test_f1_micro))\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if g is True:\n",
        "            all_glove_losses.append(test_loss)\n",
        "            all_glove_ham.append(test_hamm_loss)\n",
        "            all_glove_jacc.append(test_jacc_score)\n",
        "            all_glove_lrap.append(test_lrap)\n",
        "            all_glove_f1mac.append(test_f1_macro)\n",
        "            all_glove_f1mic.append(test_f1_micro)\n",
        "        else:\n",
        "            all_nonglove_losses.append(test_loss)\n",
        "            all_nonglove_ham.append(test_hamm_loss)\n",
        "            all_nonglove_jacc.append(test_jacc_score)\n",
        "            all_nonglove_lrap.append(test_lrap)\n",
        "            all_nonglove_f1mac.append(test_f1_macro)\n",
        "            all_nonglove_f1mic.append(test_f1_micro)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Binary Cross Entropy With Logits Loss: 0.2442\n",
            "Test Hamming Loss : 0.1645\n",
            "Test Jaccard Score: 0.3856\n",
            "Test Label Ranking Average Precision Score: 0.4845\n",
            "Test F1 Macro Score: 0.4066\n",
            "Test F1 Micro Score: 0.4622\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2670\n",
            "Test Hamming Loss : 0.1733\n",
            "Test Jaccard Score: 0.3709\n",
            "Test Label Ranking Average Precision Score: 0.4681\n",
            "Test F1 Macro Score: 0.3783\n",
            "Test F1 Micro Score: 0.4491\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2524\n",
            "Test Hamming Loss : 0.1725\n",
            "Test Jaccard Score: 0.3905\n",
            "Test Label Ranking Average Precision Score: 0.4757\n",
            "Test F1 Macro Score: 0.4214\n",
            "Test F1 Micro Score: 0.4796\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2637\n",
            "Test Hamming Loss : 0.1643\n",
            "Test Jaccard Score: 0.3823\n",
            "Test Label Ranking Average Precision Score: 0.4832\n",
            "Test F1 Macro Score: 0.3456\n",
            "Test F1 Micro Score: 0.4588\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2470\n",
            "Test Hamming Loss : 0.1705\n",
            "Test Jaccard Score: 0.3760\n",
            "Test Label Ranking Average Precision Score: 0.4744\n",
            "Test F1 Macro Score: 0.4119\n",
            "Test F1 Micro Score: 0.4513\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2653\n",
            "Test Hamming Loss : 0.1671\n",
            "Test Jaccard Score: 0.3856\n",
            "Test Label Ranking Average Precision Score: 0.4796\n",
            "Test F1 Macro Score: 0.3470\n",
            "Test F1 Micro Score: 0.4664\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2522\n",
            "Test Hamming Loss : 0.1651\n",
            "Test Jaccard Score: 0.3883\n",
            "Test Label Ranking Average Precision Score: 0.4878\n",
            "Test F1 Macro Score: 0.3880\n",
            "Test F1 Micro Score: 0.4603\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2552\n",
            "Test Hamming Loss : 0.1667\n",
            "Test Jaccard Score: 0.3823\n",
            "Test Label Ranking Average Precision Score: 0.4800\n",
            "Test F1 Macro Score: 0.3513\n",
            "Test F1 Micro Score: 0.4625\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2481\n",
            "Test Hamming Loss : 0.1623\n",
            "Test Jaccard Score: 0.3809\n",
            "Test Label Ranking Average Precision Score: 0.4852\n",
            "Test F1 Macro Score: 0.3926\n",
            "Test F1 Micro Score: 0.4555\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2683\n",
            "Test Hamming Loss : 0.1675\n",
            "Test Jaccard Score: 0.3777\n",
            "Test Label Ranking Average Precision Score: 0.4767\n",
            "Test F1 Macro Score: 0.3673\n",
            "Test F1 Micro Score: 0.4562\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2492\n",
            "Test Hamming Loss : 0.1656\n",
            "Test Jaccard Score: 0.3811\n",
            "Test Label Ranking Average Precision Score: 0.4814\n",
            "Test F1 Macro Score: 0.4026\n",
            "Test F1 Micro Score: 0.4577\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2720\n",
            "Test Hamming Loss : 0.1750\n",
            "Test Jaccard Score: 0.3685\n",
            "Test Label Ranking Average Precision Score: 0.4674\n",
            "Test F1 Macro Score: 0.3475\n",
            "Test F1 Micro Score: 0.4443\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2561\n",
            "Test Hamming Loss : 0.1702\n",
            "Test Jaccard Score: 0.3488\n",
            "Test Label Ranking Average Precision Score: 0.4573\n",
            "Test F1 Macro Score: 0.3574\n",
            "Test F1 Micro Score: 0.4251\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2542\n",
            "Test Hamming Loss : 0.1685\n",
            "Test Jaccard Score: 0.3728\n",
            "Test Label Ranking Average Precision Score: 0.4705\n",
            "Test F1 Macro Score: 0.3649\n",
            "Test F1 Micro Score: 0.4580\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2610\n",
            "Test Hamming Loss : 0.1725\n",
            "Test Jaccard Score: 0.3512\n",
            "Test Label Ranking Average Precision Score: 0.4634\n",
            "Test F1 Macro Score: 0.3616\n",
            "Test F1 Micro Score: 0.4152\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2711\n",
            "Test Hamming Loss : 0.1716\n",
            "Test Jaccard Score: 0.3747\n",
            "Test Label Ranking Average Precision Score: 0.4699\n",
            "Test F1 Macro Score: 0.3342\n",
            "Test F1 Micro Score: 0.4550\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2759\n",
            "Test Hamming Loss : 0.1760\n",
            "Test Jaccard Score: 0.3685\n",
            "Test Label Ranking Average Precision Score: 0.4650\n",
            "Test F1 Macro Score: 0.4027\n",
            "Test F1 Micro Score: 0.4473\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2638\n",
            "Test Hamming Loss : 0.1645\n",
            "Test Jaccard Score: 0.3811\n",
            "Test Label Ranking Average Precision Score: 0.4811\n",
            "Test F1 Macro Score: 0.3523\n",
            "Test F1 Micro Score: 0.4626\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2569\n",
            "Test Hamming Loss : 0.1714\n",
            "Test Jaccard Score: 0.3642\n",
            "Test Label Ranking Average Precision Score: 0.4701\n",
            "Test F1 Macro Score: 0.3673\n",
            "Test F1 Micro Score: 0.4317\n",
            "\n",
            "\n",
            "Test Binary Cross Entropy With Logits Loss: 0.2647\n",
            "Test Hamming Loss : 0.1690\n",
            "Test Jaccard Score: 0.3718\n",
            "Test Label Ranking Average Precision Score: 0.4699\n",
            "Test F1 Macro Score: 0.3490\n",
            "Test F1 Micro Score: 0.4564\n",
            "\n",
            "\n",
            "CPU times: user 43min 4s, sys: 22.4 s, total: 43min 27s\n",
            "Wall time: 43min 12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrqGW2V9EBfA",
        "outputId": "98bd53b4-f670-41d1-de0c-71d5da89a2f2"
      },
      "source": [
        "print(\"Using glove vectors:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_glove_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_glove_ham)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_glove_jacc)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_glove_lrap)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_glove_f1mac)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_glove_f1mic)/10))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Not using glove vectors:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(all_nonglove_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(all_nonglove_ham)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(all_nonglove_jacc)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(all_nonglove_lrap)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(all_nonglove_f1mac)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(all_nonglove_f1mic)/10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using glove vectors:\n",
            "Average Binary Cross Entropy With Logits Loss: 0.2543\n",
            "Average Hamming Loss : 0.1691\n",
            "Average Jaccard Score: 0.3735\n",
            "Average Label Ranking Average Precision Score: 0.4745\n",
            "Average F1 Macro Score: 0.3912\n",
            "Average F1 Micro Score: 0.4486\n",
            "\n",
            "\n",
            "\n",
            "Not using glove vectors:\n",
            "Average Binary Cross Entropy With Logits Loss: 0.2645\n",
            "Average Hamming Loss : 0.1688\n",
            "Average Jaccard Score: 0.3768\n",
            "Average Label Ranking Average Precision Score: 0.4746\n",
            "Average F1 Macro Score: 0.3538\n",
            "Average F1 Micro Score: 0.4569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJOaX46ZaNuc",
        "outputId": "40f14e12-60c0-45a2-b2ea-19f72ba33751"
      },
      "source": [
        "print(np.std(np.array(all_glove_losses)), np.std(np.array(all_glove_ham)), np.std(np.array(all_glove_jacc)), np.std(np.array(all_glove_lrap)),\n",
        "      np.std(np.array(all_glove_f1mac)), np.std(np.array(all_glove_f1mic)))\n",
        "\n",
        "print(np.std(np.array(all_nonglove_losses)), np.std(np.array(all_nonglove_ham)), np.std(np.array(all_nonglove_jacc)), np.std(np.array(all_nonglove_lrap)),\n",
        "      np.std(np.array(all_nonglove_f1mac)), np.std(np.array(all_nonglove_f1mic)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0086561786004063 0.004182697536003043 0.014116278430858767 0.009855138493442476 0.021058861987352975 0.01837339060432787\n",
            "0.005628179990185498 0.0033821236421187357 0.005544719245186052 0.005750635721870256 0.01212804528633011 0.006188793398098033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs-V8hnPiA9h"
      },
      "source": [
        "# Comparing a unidirectional and a bidirectional network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OxY-zFbg9-u",
        "outputId": "99080d6b-14f0-41bf-8f82-76c02434c1a6"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "hidden_dim = 128\n",
        "threshold = 0.5\n",
        "drop_prob = 0.65\n",
        "\n",
        "iter = 1\n",
        "bce_losses = []\n",
        "ham_losses = []\n",
        "jacc_scores = []\n",
        "lrap_scores = []\n",
        "f1_macros = []\n",
        "f1_micros = []\n",
        "\n",
        "bi_bce_losses = []\n",
        "bi_ham_losses = []\n",
        "bi_jacc_scores = []\n",
        "bi_lrap_scores = []\n",
        "bi_f1_macros = []\n",
        "bi_f1_micros = []\n",
        "\n",
        "for exp in range(1, 11):\n",
        "    for dirn in [True, False]:\n",
        "        model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, \n",
        "                            drop_prob = drop_prob, bidirectional = dirn, use_glove = True)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "        test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "        \n",
        "        if dirn is True:\n",
        "            bi_bce_losses.append(test_loss)\n",
        "            bi_ham_losses.append(test_hamm_loss)\n",
        "            bi_jacc_scores.append(test_jacc_score)\n",
        "            bi_lrap_scores.append(test_lrap)\n",
        "            bi_f1_macros.append(test_f1_macro)\n",
        "            bi_f1_micros.append(test_f1_micro)\n",
        "        else:\n",
        "            bce_losses.append(test_loss)\n",
        "            ham_losses.append(test_hamm_loss)\n",
        "            jacc_scores.append(test_jacc_score)\n",
        "            lrap_scores.append(test_lrap)\n",
        "            f1_macros.append(test_f1_macro)\n",
        "            f1_micros.append(test_f1_micro)\n",
        "\n",
        "        print(\"Iteration {} / {} done\".format(iter, 20))\n",
        "        iter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 / 20 done\n",
            "Iteration 2 / 20 done\n",
            "Iteration 3 / 20 done\n",
            "Iteration 4 / 20 done\n",
            "Iteration 5 / 20 done\n",
            "Iteration 6 / 20 done\n",
            "Iteration 7 / 20 done\n",
            "Iteration 8 / 20 done\n",
            "Iteration 9 / 20 done\n",
            "Iteration 10 / 20 done\n",
            "Iteration 11 / 20 done\n",
            "Iteration 12 / 20 done\n",
            "Iteration 13 / 20 done\n",
            "Iteration 14 / 20 done\n",
            "Iteration 15 / 20 done\n",
            "Iteration 16 / 20 done\n",
            "Iteration 17 / 20 done\n",
            "Iteration 18 / 20 done\n",
            "Iteration 19 / 20 done\n",
            "Iteration 20 / 20 done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmuZn0SORSfZ",
        "outputId": "2982f600-7756-43b4-966f-2eff311dfbe4"
      },
      "source": [
        "print(\"Using bidirectional lstm:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(bi_bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(bi_ham_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(bi_jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(bi_lrap_scores)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(bi_f1_macros)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(bi_f1_micros)/10))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Using lstm:\")\n",
        "print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(bce_losses)/10))\n",
        "print(\"Average Hamming Loss : {:.4f}\".format(sum(ham_losses)/10))\n",
        "print(\"Average Jaccard Score: {:.4f}\".format(sum(jacc_scores)/10))\n",
        "print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(lrap_scores)/10))\n",
        "print(\"Average F1 Macro Score: {:.4f}\".format(sum(f1_macros)/10))\n",
        "print(\"Average F1 Micro Score: {:.4f}\".format(sum(f1_micros)/10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using bidirectional lstm:\n",
            "Average Binary Cross Entropy With Logits Loss: 0.2884\n",
            "Average Hamming Loss : 0.1816\n",
            "Average Jaccard Score: 0.3663\n",
            "Average Label Ranking Average Precision Score: 0.4573\n",
            "Average F1 Macro Score: 0.4001\n",
            "Average F1 Micro Score: 0.4487\n",
            "\n",
            "\n",
            "\n",
            "Using lstm:\n",
            "Average Binary Cross Entropy With Logits Loss: 0.2510\n",
            "Average Hamming Loss : 0.1658\n",
            "Average Jaccard Score: 0.3841\n",
            "Average Label Ranking Average Precision Score: 0.4829\n",
            "Average F1 Macro Score: 0.4008\n",
            "Average F1 Micro Score: 0.4608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UV0PyySgE4Q",
        "outputId": "3214d46b-df35-4bf7-99ea-180fa5ec289f"
      },
      "source": [
        "print(\"Using bidirectional lstm:\")\n",
        "print(\"Deviation of Binary Cross Entropy With Logits Loss: {:.4f}\".format(np.std(np.array(bi_bce_losses))))\n",
        "print(\"Deviation of Hamming Loss : {:.4f}\".format(np.std(np.array(bi_ham_losses))))\n",
        "print(\"Deviation of Jaccard Score: {:.4f}\".format(np.std(np.array(bi_jacc_scores))))\n",
        "print(\"Deviation of Label Ranking Average Precision Score: {:.4f}\".format(np.std(np.array(bi_lrap_scores))))\n",
        "print(\"Deviation of F1 Macro Score: {:.4f}\".format(np.std(np.array(bi_f1_macros))))\n",
        "print(\"Deviation of F1 Micro Score: {:.4f}\".format(np.std(np.array(bi_f1_micros))))\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "print(\"Using lstm:\")\n",
        "print(\"Deviation of Binary Cross Entropy With Logits Loss: {:.4f}\".format(np.std(np.array(bce_losses))))\n",
        "print(\"Deviation of Hamming Loss : {:.4f}\".format(np.std(np.array(ham_losses))))\n",
        "print(\"Deviation of Jaccard Score: {:.4f}\".format(np.std(np.array(jacc_scores))))\n",
        "print(\"Deviation of Label Ranking Average Precision Score: {:.4f}\".format(np.std(np.array(lrap_scores))))\n",
        "print(\"Deviation of F1 Macro Score: {:.4f}\".format(np.std(np.array(f1_macros))))\n",
        "print(\"Deviation of F1 Micro Score: {:.4f}\".format(np.std(np.array(f1_micros))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using bidirectional lstm:\n",
            "Deviation of Binary Cross Entropy With Logits Loss: 0.0083\n",
            "Deviation of Hamming Loss : 0.0049\n",
            "Deviation of Jaccard Score: 0.0120\n",
            "Deviation of Label Ranking Average Precision Score: 0.0094\n",
            "Deviation of F1 Macro Score: 0.0109\n",
            "Deviation of F1 Micro Score: 0.0140\n",
            "\n",
            "\n",
            "\n",
            "Using lstm:\n",
            "Deviation of Binary Cross Entropy With Logits Loss: 0.0077\n",
            "Deviation of Hamming Loss : 0.0038\n",
            "Deviation of Jaccard Score: 0.0114\n",
            "Deviation of Label Ranking Average Precision Score: 0.0080\n",
            "Deviation of F1 Macro Score: 0.0139\n",
            "Deviation of F1 Micro Score: 0.0121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX7dndlfnQrD"
      },
      "source": [
        "we see that a bidirectional network performs much worse than a unidirectional network with the same set of parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq0-UjGljpmd"
      },
      "source": [
        "# Comparing dropout values of 0, 0.5, 0.6, 0.7 for adding in the table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q44D6Hqjwp1",
        "outputId": "35d32026-51f5-41b9-c22b-d6423eb8897d"
      },
      "source": [
        "learning_rates = [1e-3]\n",
        "hidden_dims = [128]\n",
        "drop_probs = [0, 0.5, 0.6, 0.7]\n",
        "\n",
        "for drop_prob in drop_probs:\n",
        "    bce_losses = []\n",
        "    ham_losses = []\n",
        "    jacc_scores = []\n",
        "    lrap_scores = []\n",
        "    f1_macros = []\n",
        "    f1_micros = []\n",
        "    for exp in range(1, 11):\n",
        "        model = CustomLSTM(embedding_dim = vocab.vectors.shape[1], vocab = vocab, hidden_dim = hidden_dim, output_dim = 11, drop_prob = drop_prob)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "        loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train_loss, hamm_loss, jacc_score, lrap, f1_macro, f1_micro = train(model, loss_fn, optimizer, 50, train_dl, threshold)\n",
        "        #all_models.append(model)\n",
        "\n",
        "        test_loss, test_hamm_loss, test_jacc_score, test_lrap, test_f1_macro, test_f1_micro = test(model, loss_fn, test_dl, threshold)\n",
        "        bce_losses.append(test_loss)\n",
        "        ham_losses.append(test_hamm_loss)\n",
        "        jacc_scores.append(test_jacc_score)\n",
        "        lrap_scores.append(test_lrap)\n",
        "        f1_macros.append(test_f1_macro)\n",
        "        f1_micros.append(test_f1_micro)\n",
        "    \n",
        "    print(\"Dropout = {}\".format(drop_prob))\n",
        "    print(\"Average Binary Cross Entropy With Logits Loss: {:.4f}\".format(sum(bce_losses)/10))\n",
        "    print(\"Average Hamming Loss : {:.4f}\".format(sum(ham_losses)/10))\n",
        "    print(\"Average Jaccard Score: {:.4f}\".format(sum(jacc_scores)/10))\n",
        "    print(\"Average Label Ranking Average Precision Score: {:.4f}\".format(sum(lrap_scores)/10))\n",
        "    print(\"Average F1 Macro Score: {:.4f}\".format(sum(f1_macros)/10))\n",
        "    print(\"Average F1 Micro Score: {:.4f}\".format(sum(f1_micros)/10))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    print(\"Deviation of Binary Cross Entropy With Logits Loss: {:.4f}\".format(np.std(np.array(bce_losses))))\n",
        "    print(\"Deviation of Hamming Loss : {:.4f}\".format(np.std(np.array(ham_losses))))\n",
        "    print(\"Deviation of Jaccard Score: {:.4f}\".format(np.std(np.array(jacc_scores))))\n",
        "    print(\"Deviation of Label Ranking Average Precision Score: {:.4f}\".format(np.std(np.array(lrap_scores))))\n",
        "    print(\"Deviation of F1 Macro Score: {:.4f}\".format(np.std(np.array(f1_macros))))\n",
        "    print(\"Deviation of F1 Micro Score: {:.4f}\".format(np.std(np.array(f1_micros))))\n",
        "    print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropout = 0\n",
            "Average Binary Cross Entropy With Logits Loss: 0.7492\n",
            "Average Hamming Loss : 0.2223\n",
            "Average Jaccard Score: 0.3267\n",
            "Average Label Ranking Average Precision Score: 0.4079\n",
            "Average F1 Macro Score: 0.3690\n",
            "Average F1 Micro Score: 0.4083\n",
            "\n",
            "\n",
            "\n",
            "Deviation of Binary Cross Entropy With Logits Loss: 0.0318\n",
            "Deviation of Hamming Loss : 0.0072\n",
            "Deviation of Jaccard Score: 0.0108\n",
            "Deviation of Label Ranking Average Precision Score: 0.0089\n",
            "Deviation of F1 Macro Score: 0.0142\n",
            "Deviation of F1 Micro Score: 0.0125\n",
            "\n",
            "\n",
            "\n",
            "Dropout = 0.5\n",
            "Average Binary Cross Entropy With Logits Loss: 0.3029\n",
            "Average Hamming Loss : 0.1800\n",
            "Average Jaccard Score: 0.3690\n",
            "Average Label Ranking Average Precision Score: 0.4616\n",
            "Average F1 Macro Score: 0.4001\n",
            "Average F1 Micro Score: 0.4484\n",
            "\n",
            "\n",
            "\n",
            "Deviation of Binary Cross Entropy With Logits Loss: 0.0142\n",
            "Deviation of Hamming Loss : 0.0053\n",
            "Deviation of Jaccard Score: 0.0104\n",
            "Deviation of Label Ranking Average Precision Score: 0.0091\n",
            "Deviation of F1 Macro Score: 0.0098\n",
            "Deviation of F1 Micro Score: 0.0115\n",
            "\n",
            "\n",
            "\n",
            "Dropout = 0.6\n",
            "Average Binary Cross Entropy With Logits Loss: 0.2668\n",
            "Average Hamming Loss : 0.1716\n",
            "Average Jaccard Score: 0.3789\n",
            "Average Label Ranking Average Precision Score: 0.4751\n",
            "Average F1 Macro Score: 0.3960\n",
            "Average F1 Micro Score: 0.4549\n",
            "\n",
            "\n",
            "\n",
            "Deviation of Binary Cross Entropy With Logits Loss: 0.0074\n",
            "Deviation of Hamming Loss : 0.0043\n",
            "Deviation of Jaccard Score: 0.0123\n",
            "Deviation of Label Ranking Average Precision Score: 0.0091\n",
            "Deviation of F1 Macro Score: 0.0138\n",
            "Deviation of F1 Micro Score: 0.0152\n",
            "\n",
            "\n",
            "\n",
            "Dropout = 0.7\n",
            "Average Binary Cross Entropy With Logits Loss: 0.2407\n",
            "Average Hamming Loss : 0.1629\n",
            "Average Jaccard Score: 0.3809\n",
            "Average Label Ranking Average Precision Score: 0.4848\n",
            "Average F1 Macro Score: 0.3890\n",
            "Average F1 Micro Score: 0.4547\n",
            "\n",
            "\n",
            "\n",
            "Deviation of Binary Cross Entropy With Logits Loss: 0.0068\n",
            "Deviation of Hamming Loss : 0.0036\n",
            "Deviation of Jaccard Score: 0.0143\n",
            "Deviation of Label Ranking Average Precision Score: 0.0112\n",
            "Deviation of F1 Macro Score: 0.0112\n",
            "Deviation of F1 Micro Score: 0.0132\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgYWErjDC_sS",
        "outputId": "106bd5a8-fb15-4bdf-bc8d-89f2fee8942a"
      },
      "source": [
        "print(\"BCE Losses: \", bce_losses)\n",
        "print(\"Hamming Losses: \", ham_losses)\n",
        "print(\"Jaccard Scores: \", jacc_scores)\n",
        "print(\"LRAP Scores: \", lrap_scores)\n",
        "print(\"F1 Macros: \", f1_macros)\n",
        "print(\"F1 Micros: \", f1_micros)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCE Losses:  [0.24093458206951618, 0.2435429848879576, 0.24597274278104306, 0.2317008166909218, 0.24469883343577384, 0.2420098347812891, 0.2549565038233995, 0.23544572694599628, 0.23294113199412822, 0.235086762920022]\n",
            "Hamming Losses:  [0.16363636363636364, 0.16004545454545455, 0.1674090909090909, 0.15763636363636363, 0.16668181818181818, 0.15995454545454546, 0.1685, 0.1585, 0.16318181818181818, 0.16372727272727272]\n",
            "Jaccard Scores:  [0.3567000000000001, 0.3773916666666666, 0.3868666666666667, 0.4032916666666666, 0.3748916666666667, 0.37606666666666666, 0.3749416666666666, 0.402425, 0.39124999999999993, 0.36504999999999993]\n",
            "LRAP Scores:  [0.47092058080807647, 0.4829508838383814, 0.4848305555555535, 0.5047712121212098, 0.4790854797979772, 0.4826655303030278, 0.47704911616161394, 0.5020186868686849, 0.4928306818181795, 0.4706079545454523]\n",
            "F1 Macros:  [0.368326461462595, 0.380443216938662, 0.4045637561647114, 0.3999672956880074, 0.38521084262414873, 0.38004355561130226, 0.38081506962603306, 0.402522276272099, 0.39495648500521285, 0.3933886184452419]\n",
            "F1 Micros:  [0.4274809160305344, 0.45655193702731905, 0.4605243884575948, 0.47390776699029125, 0.4476577797861124, 0.4533167624669877, 0.44729387207395255, 0.4755602346217477, 0.4568835098335855, 0.4478847332924586]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "EMRZaTbWpnBn",
        "ycqWYfXQFzjN",
        "H_sgcXIp7Odd",
        "G8gsgspsMt23",
        "buDnFn21kxoo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}